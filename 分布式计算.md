*拒绝撕逼！！！
本片属于论文翻译，此翻译仅仅是为了学习，加上水平有限，见谅！*

【原文】**《MapReduce: Simplified Data Processing on Large Clusters》**，作者**Jeffrey Dea**n 和 **Sanjay Ghemawat**

# 分布式计算：简化大规模集群下的数据处理

## 摘要
分布式计算（MapReduce）是一个处理、生成大规模数据集的编程模型及其相关实现。用户指定一个`map`函数来处理键值对从而生成一组中间键值对，一个`reduce`函数用来合并中间键相同的所有值。正如本文中所示，很多现实中的任务都可以在这个模型中表达出来。

这种函数式编程可以自动的并行化，并在商用机器组成的大规模集群中执行。运行时系统注重输入数据的分割，一组机器之间程序执行的调度，故障机器的处理，所需机器间的通讯管理等这些细节。这允许没有并行和分布式系统经验的程序员可以轻松地利用大型分布式系统资源。

我们的分布式计算实现运行在由商用机器组成的大规模集群中且具有高可扩展性：一个典型的分布式计算过程需要在数以千计的机器上处理数兆兆字节的数据。程序员会发现系统使用很简单：现在已经有数以百计种分布式计算程序的实现方式，并且每天都有超过一千个分布式计算任务在Google的集群上执行。

---

## 1 简介
在过去的五年中，作者和很多其他Google的工作人员已经实现了数以百计有特殊用途的计算，这种计算会处理海量的原始数据，例如抓取的文档，网络请求日志等等，以计算出各种衍生数据，例如倒排索引，各种表征web文档的图结构，每个主机抓取的页面数摘要，一天中查询最频繁的关键字集合等等。从概念上讲，这样的计算大多数都很简单。然而，输入数据量通常会很大，并且计算必须分布在数百或者数千台机器上，以便在合理的时间内完成。如何进行并行计算，分发数据和处理故障，正是为了解决这些问题而使用了大量的复杂代码掩盖了原始的简单计算。

为了应对这种复杂性，我们设计了一个新的抽象概念，它允许我们表达我们想要执行的简单计算，但是隐藏了在一个库中并行，容错，数据分发，负载均衡等繁琐的细节。我们的抽象概念的灵感来源于Lisp和很多其他函数式语言中的内置函数`map`和`reduce`。我们意识到很多计算都涉及到向我们输入的每一个合理的“记录”都应用一个`map`操作来计算出一组中间键值对，向拥有共同键的所有值都应用一个`reduce`操作来适当的合并衍生出来的数据。我们使用带有用户指定`map`和`reduce`操作的函数模型能让我们轻松地进行大规模并行计算，并把重新执行作为主要的容错机制。

这项工作的主要贡献是提供了简单且强大的能够进行自动并行和大规模分布式计算的接口，结合该接口的实现，在大规模商用计算机集群上有着不俗的性能表现。

第二部分描述了基本的编程模型并给出了几个示例。第三部分描述了一个集群计算环境的分布式计算接口的实现。第四部分描述了我们发现的一个有用的改进编程模型。第五部分对各种任务的实现进行了性能评测。第六部分探讨了分布式计算在Google中的使用，包括我们将其作为基础而重写产品索引系统的经验。第七部分讨论了相关的和未来的工作。

---

## 2 编程模型
计算接受一组键值对作为输入，并生成一组键值对作为输出。分布式计算库的用户将计算表示为两个函数：*Map*和*Reduce*。

用户编写的*Map*函数接受一个输入对并生成一组中间键值对。分布式计算库把与中间key值 ***I*** 相同的所有中间value值组合到一起并传递给*Reduce*方法。

用户编写的*Reduce*函数接收一个中间key值 ***I*** 和一组与此key值相关的value值。他会合并这些值产生规模更小的一组value值。通常的，每一次*Reduce*调用只会生成0或者1个输出值。中间值通过迭代器把值提供给用户的*Reduce*函数。这样可以允许我们处理大到内存无法容纳的值列表。

### 2.1 示例
想一下这个问题：计算一下在一个大的文档集中每个单词出现的次数。用户可能会写下如下的伪代码：
```
map(String key, String value):
	// key: 文档名称
	// value: 文档内容
	for each word w in value:
		EmitIntermediate(w, "1");
		
reduce(String key, Iterator values):
	// key: 一个单词
	// values: 计数列表
	int result = 0;
	for each v in values:
		result += ParseInt(v);
	Emit(AsString(result));
```
`map`函数返回一个单词和与其相关出现次数（在这个例子中只为“1”）。`reduce`函数统计了一个特定单词返回的所有计数。

此外，用户编码用输入、输出文件的名称和可选的调优参数来满足一个符合`MapReduce`规范的对象。然后用户调用`MapReduce`函数，并传递给它一个规范的对象。让用户的代码与`MapReduce`库（由C++实现）连接在一起。附录A包含该示例的完整代码。

### 2.2 类型
尽管上面的伪代码的输入输出类型是`string`，但是，从概念上讲，用户提供的`map`和`reduce`函数都有与其对应的类型：
```
map(k1, v1)			         ---> list(k2, v2)
reduce(k2, list(v2))		----> list(v2)
```
也就是说，输入的键和值和输出的键和值来自不同的域。此外，中间键和值和输出的键和值来自相同的域。

我们的C++实现把字符串从用户定义的函数中传入和输出，并将其留给用户代码以让它在字符串和适当类型之间转换。


### 2.3 更多示例
下面有几个简单有趣的程序示例，可以很容易的将它们表示为分布式计算。

**Distributed Grep（正则表达式）：**如果输入值匹配给定的模式，`map`函数会输出一行。`reduce`函数是一个识别函数，仅仅把提供的中间函数拷贝到输出。

**统计URL访问频率：**`map`函数处理网页请求日志，并输出`<UEL, 1>`。`reduce`函数把所有URL相同的值累加起来并输出`<URL, total count>`数据对。

**反转网页链接图：**`map`函数为每一个在名为`source`的页面中能连接到`target` URL的链接输出`<target, source>`数据对。`reduce`函数会连接所有与给定target URL相关source URLs列表，并输出`<target, list<source>>`对。

**主机关键向量指标：**一个检索词向量摘录了出现在一个或一组文档中最重要的单词作为`<word, frequency>`列表。`map`函数为每一个输入文档（其中主机名是从文档的URL中提取的）输出一个`<hostname, term vector>`对。`reduce`函数接收给定的主机所有的单文档检索词向量。它会把这些检索词向量聚合在一起，丢弃低频检索词，然后输出最终的`<hostname, term vector>`数据对。

**倒排索引：**`map`函数对每一个文案的进行解析，并输出一系列`<word, document ID>`数据对。`reduce`函数接受一个单词对应的所有键值对，对这些文档id进行排序并输出一个`<word, list(document ID)>`数据对。所有的输出键值对集合组成了一个简单的倒排索引。通过增大计算量很容易的就可以跟踪单词的位置。

**分布式排序：**`map`函数从每一条记录中提取键，并输出`<key, record>`数据对。`reduce`函数输出所有未改变的键值对。这个计算依赖于4.1章节介绍的分割工具和4.2章介绍的排序特性。

## 实现
很多种分布式计算接口的实现都很合理。正确的选择取决于你所应用的场景。例如，有的实现可能适合小共享内存机器，有的则适合大型非一致内存访问（NUMA）多处理器，还有的则适合更大型的联网服务器集群。

这节介绍谷歌广泛使用的计算环境的实现：以交换式以太网[4]连接起来的大型商用PC集群。在我们的环境中：
（1）机器通常是运行着Linux的双核x86处理器，每个机器有2-4GB的内存。
（2）通常使用机器级别为100M/s或1GB/s的商用网络硬件，但所有平均对分带宽都要小的多。
（3）一个集群有成百上千台机器组成，因此机器故障也极为常见。
（4）存储由直接连接在单个机器上的廉价硬盘提供。内部研发的分布式文件系统[8]用来管理存储在这些硬盘上的数据。文件系统用复制来在不可靠的硬件上提供实用性、可靠性。
（5）用户把工作提交给调度系统。每一个工作由一组任务组成，并由调度程序映射到一组可用的机器上。

### 3.1 执行概述
通过自动把输入数据切分为M片，使得`Map`调用分布在多台机器上。输入的分片数据可以有不同的机器并行处理。通过一个分割函数（例如：hash(key)**mod** R）把上一步生成的中间key分成R片，使得`Reduce`调用以分布式的方式进行。分割数量R和分割函数由用户指定。
![图一](https://github.com/singmiya/translate/blob/master/datas/excution_overview.png)

图一展示了我们实现的*MapReduce*操作的总体流程。当用户程序调用*MapReduce*函数的时候，会按照下面的顺序执行动作（图一中编号的标签与下面列表中的数字相对应）：

1. 在用户程序中的*MapReduce*库首先会把输入文件分割为每块大小通常为16百万字节到64百万字节（由用户通过一个可选的参数控制）的M块数据。然后在机器集群上启动该程序的多个副本。
2. 程序的每一个副本都是特别的——Master。剩下的就是由Master分配工作的Worker了。现在有M个`map`任务和R个`reduce`任务需要分配。Master选择空闲的Worker并给他分配一个`map`或者`reduce`任务。
3. 分配到`map`任务的Worker从相应的输入分/切片中读取内容。它会把输入的数据解析成键值对并把每一个键值对传递给用户自定义的*Map*函数。这样*Map*函数就会产生中间键值对并把它们缓存在内存中。
4. 缓存的键值对会被定期的写入到本地磁盘，并由分区函数分成R个区域。在本地磁盘中的缓存地址会返回给Master，并由Master负责把这些地址转发给`reduce`Worker。
5. 当Master通知`reduce`Worker这些地址信息时，它会使用远程过程调用去从`map`“Worker的本地磁盘中读取缓存数据。当`reduce`Worker读取了所有的中间数据后，按照key对其排序，这样所有拥有相同key值的数据就聚合到了一块儿。通常，排序是有必要的因为不排序会有很多不同的key映射到同一个`reduce`任务上。如果中间数据量太大使得内存中无法放下，那么就需要进行外部排序了。
6. `reduce`任务会遍历排序过的中间数据，每遍历一个唯一中间key，它都会把key和其对应的一组中间值传递给用户的*Reduce*函数。*Reduce*函数都会把输出数据附加在`reduce`分区的最终输出文件中。
7. 当所有的`map`任务和`reduce`任务都结束后，Master会唤醒用户程序。这时，在用户程序中的*MapReduce*调用会返回到用户代码。

成功完成后，`mapreduce`执行后的输出是R个可用的文件（每一个`reduce`任务一个，文件名称由用户指定）。通常，用户不需要把R个文件合成一个——它们通常把这R个输出文件当做另外一个*Mapreduce*调用的输入，或者在其他的需要把一个文件分割成多个分布式应用中使用它们。

### 3.2 主数据结构
Master有几种数据结构。对于每一个`map`和`reduce`任务，它保存状态（空闲，执行中或者完成）和Worker机器（非空闲的任务）的标识。

Master是把中间文件分区的位置从`map`任务传递到`reduce`任务的管道。因此，对于每一个完成的`map`任务，Master存储由`map`任务产生的R个中间文件分区的位置和大小。`map`任务完成后，Master会接受到位置和大小信息的更新通知。信息会逐渐的推送给正在执行`reduce`任务的Worker。

### 3.3 故障容错
由于`MapReduce`库是被设计用来在成百上千台机器上处理海量数据的，所以，这个库要有适当的容忍机器故障。

#### Worker故障
Master会周期性的与每一个Worker通信。如果没有在确定的时间内接收到Worker的响应，Master会把Worker标记为故障。任何一个由Worker完成的`map`任务都会重置为初始的空闲状态，这样，才可以在其他Worker上调度。同样的，任何在故障Worker中正在执行的`map`或者`reduce`任务也会被设为空闲并且可以从新调度。

已完成的`map`任务在失败时会再重新执行，因为他们的输出存储在了故障机器的本地磁盘中而导致了无法获取。已完成的`reduce`任务则不需要，因为他们的输出文件存储到了全局文件系统中。

当一个`map`任务先由Worker A执行后又由Worker B（因为A失败了）执行时，所有执行`reduce`任务的Worker都会接受到（map任务）重新执行的通知。任何还没有准备从Worker A读取数据的`reduce`任务都将会从Worker B读取数据。

*MapReduce*对大规模Worker故障有灵活的应变能力。例如，在一次*MapReduce*操作期间，正在运行集群上的网络维护导致80台机器群组有几分钟无法访问。*MapReduce* Master只需简单地重新执行一下无法访问的Worker机器完成的任务就可以继续推进工作进行，最终完成*MapReduce*操作。

#### Master故障
 对上面介绍的Master数据结构的检查点进行定期保存，Master可以轻易完成。如果Master任务出现故障，一份新的副本可以从上一个检查点状态重新开始。然而，如果只有单个Master，它出现故障的话那就不妙了；因此，如果Master出现故障，我们当前的实现会终止*MapReduce*计算。用户可以检查这种情况，如果有愿意的话可以重试*MapReduce*操作。
 
#### 故障情况下的语义
当用户提供的`map`和`reduce`操作符是其输入值的确定性函数时，我们的分布式实现与整个程序的非故障顺序执行产生的输出是一样的。

我们依赖`map`和`reduce`任务输出的原子提交操作来实现这一特性。每一个执行中的任务都会把它的输出写入一个私有的临时文件中。一个`reduce`任务产生一个这样的文件，一个`map`任务则产生R个这样的文件（每一个`reduce`任务产生一个）。当一个`map`任务完成时，Worker会向Master发送一个包含R个临时文件名称的消息。如果Master接收到的一个已经完成`map`任务的完成消息，Master会将其忽略。否则，Master会把R个文件的名字记录到一个主数据结构中。

当一个`reduce`任务完成时，`reduce` Worker会自动把临时文件重命名为最终输出文件。如果多台机器上执行了相同的`reduce`任务，那就会对同一个最终输出文件进行多次重命名操作。我们依赖底层文件系统提供的重命名操作来保证最终的文件系统状态仅仅包含一次`reduce`任务执行产生的数据。

我们绝大多数的`map`和`reduce`操作符都是具有确定性的，并且事实上，我们的语义相当于这种情况下的顺序执行，这使得程序员很容易推断他们程序的行为。当`map`和（或者）`reduce`操作符不具有确定性时，我们提供弱化但合理的语义。在存在非确定性性的操作符的情况下，特定`reduce`任务$R_1$的输出与顺序执行非确定性的程序产生的$R_1$输出是等价的。然而，不同`reduce`任务$R_2$的输出可能会与不同非确定性程序的顺序执行产出的R2输出相对应。

想一下，`map`任务$M$和`reduce`任务$R_1$和$R_2$。让$e(R_i)$	当做提交$R_i$的执行函数（只有一个这样的执行函数）。这时，弱语义就会出现，因为$e(R_i)$需要从$M$的一次执行或者$M$的不同执行产生的输出中读取数据。

### 3.4 局限性
在我们的计算环境中网络带宽是一个相关的稀缺资源。我们通过利用数据（由GFS[8]管理）保存在组成集群的计算机本地磁盘上这样的一个事实，来达到节省网络带宽的目的。GFS把每个文件分割成64M的数据块，并把每个块的几个副本（通常是3个副本）保存在不同的机器上。*MapReduce* Master会把输入文件的位置信息考虑在内，并尝试在一个包含一份对应输入数据的机器上调度`map`任务。如果失败的话，它会尝试在该任务输入数据的副本附近调度一个`map`任务（例如，在与包含数据的计算机同一个网络的工作机）。当在急群众的关键工作机上运行大型`MapReduce`操作的时候，大多数输入数据都是从本地读取且不耗费网络带宽。

### 3.5 任务粒度




